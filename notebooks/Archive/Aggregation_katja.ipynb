{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 20)\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as mgn\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/full_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page106523 = df.query('page_id == 106523')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Concat the authors by `page_id`, `day`, `publish_date`, `word_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_row', 139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page106523_aggr = page106523[['page_id', 'date', 'publish_date', 'word_count', 'authors']]\n",
    "page106523_aggr.loc[:, 'authors'] = page106523_aggr.authors.apply(lambda s: s.replace(' und ', ';'))\n",
    "\n",
    "page106523_aggr = page106523_aggr[['page_id', 'date', 'publish_date', 'word_count', 'authors']]\\\n",
    "    .groupby(['page_id', 'date', 'publish_date', 'word_count']) \\\n",
    "        .agg(lambda auth: ';'.join(auth))\n",
    "\n",
    "page106523_aggr = page106523_aggr.authors.apply(lambda auths: sorted(list(set(auths.split(';'))))).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggr = df[['page_id', 'date', 'publish_date', 'word_count', 'authors']]\n",
    "df_aggr.loc[:, 'authors'] = df_aggr.authors.apply(lambda s: s.replace(' und ', ';'))\n",
    "\n",
    "df_aggr = df_aggr[['page_id', 'date', 'publish_date', 'word_count', 'authors']]\\\n",
    "    .groupby(['page_id', 'date', 'publish_date', 'word_count']) \\\n",
    "        .agg(lambda auth: ';'.join(auth))\n",
    "\n",
    "df_aggr = df_aggr.authors.apply(lambda auths: sorted(list(set(auths.split(';'))))).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df.drop(['authors'], axis=1), \n",
    "        df_aggr,\n",
    "        on=['page_id', 'date', 'publish_date', 'word_count'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1014826 = df.query('page_id == 1014826')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recalculate the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'authors'] = df.loc[:, 'authors'].apply(lambda l: str(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[:, 'authors'] = df.authors.str.removeprefix(\"['\")\n",
    "#df.loc[:, 'authors'] = df.authors.str.removesuffix(\"']\")\n",
    "\n",
    "### This should do the same thing!\n",
    "df.authors.apply(''.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[['page_id', 'word_count', 'publish_date', 'authors']].copy()\n",
    "temp.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.drop_duplicates()\n",
    "temp = temp.sort_values('publish_date')\n",
    "\n",
    "wc_versions = temp.groupby('page_id')['word_count'].transform(lambda x: pd.factorize(x)[0])\n",
    "publish_versions = temp.groupby('page_id')['publish_date'].transform(lambda x: pd.factorize(x)[0])\n",
    "authors_versions = temp.groupby('page_id')['authors'].transform(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "version_count = 10000*wc_versions + 1000*publish_versions + 1*authors_versions\n",
    "temp['ver_id_wc'] = wc_versions\n",
    "temp['ver_id_pub'] = publish_versions\n",
    "temp['ver_id_auth'] = authors_versions\n",
    "temp['version_id_raw'] = version_count\n",
    "temp['version_id_new'] = temp.groupby('page_id')['version_id_raw'].transform(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "df_versions = pd.merge(left=df, right=temp.drop(['ver_id_wc', 'ver_id_pub', 'ver_id_auth', 'version_id_raw'], axis=1),\n",
    "         on=['page_id', 'word_count', 'publish_date', 'authors'],\n",
    "         how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page106523 = df_versions.query('page_id == 106523')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('../data/aggr_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at **1010557**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation on a page_id level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 1: external_impressions and external_clicks:\n",
    "+ page_id\n",
    "+ date\n",
    "\n",
    "Group 2: video_play, page_impressions, clickouts:\n",
    "+ page_id\n",
    "+ date\n",
    "+ URL\n",
    "+ Author (edge case)\n",
    "\n",
    "Group 3: daily_likes, daily_dislikes:\n",
    "+ page_id\n",
    "+ date\n",
    "+ publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_col = df_new[['page_id', 'date', 'url']]\n",
    "\n",
    "url_col = url_col[['page_id', 'date', 'url']]\\\n",
    "    .groupby(['page_id', 'date']) \\\n",
    "        .agg(lambda url: ';'.join(url))\n",
    "\n",
    "url_col = url_col.url.apply(lambda urls: sorted(list(set(urls.split(';')))))\n",
    "url_col = url_col.apply(','.join).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_url = df_new[['page_id', 'date', 'url', 'video_play', 'page_impressions', 'clickouts', 'version_id_new']]\\\n",
    "                .groupby(['page_id', 'date', 'url']).max()\n",
    "\n",
    "df_agg_page_date_pubdate =  df_new[['page_id', 'date', 'publish_date', 'daily_likes', 'daily_dislikes']]\\\n",
    "               .groupby(['page_id', 'date', 'publish_date']).sum()\n",
    "\n",
    "df_agg_page_date = df_new.drop(['publish_date', 'old_index', 'url', 'video_play', \n",
    "                                'version_id_new', 'version_id', 'word_count', 'page_impressions', 'clickouts', \n",
    "                               'daily_likes', 'daily_dislikes'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_url.shape, df_agg_page_date_pubdate.shape, df_agg_page_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_page_date.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full_0 = pd.merge(left=df_new[['page_id', 'date']].drop_duplicates(), \n",
    "                         right=url_col, on=['page_id', 'date'])\n",
    "df_agg_full_1 = pd.merge(left=df_agg_full_0, right=df_agg_page_date, on=['page_id', 'date'], how='inner')\n",
    "df_agg_full_2 = pd.merge(left=df_agg_full_1, right=df_agg_page_date_pubdate, on=['page_id', 'date'], how='inner')\n",
    "df_agg_full_3 = pd.merge(left=df_agg_full_2, right=df_agg_url, on=['page_id', 'date', 'url'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgn.matrix(df_agg_full_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_full = pd.merge(left=df_agg_page_date, right=df_agg_page_date_pubdate, on=['page_id', 'date'], how='inner')\n",
    "#df_agg_full = pd.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How frequently the article was published?\n",
    "\n",
    "How to catch it:\n",
    "1. Identify \"first\" dates\n",
    "    * Perhaps, difference btw dates?\n",
    "* Min, max, avg distance btw dates\n",
    "* Number of \"first\" dates (= peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/data_aggr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c227187abb74291a6be455b8ff8f50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='<div id=\"ifr-pyg-000615d67d988b32QYhFCsTKtogM5uZ6\" style=\"height: auto\">\\n    <head>â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pygwalker as pyg\n",
    "\n",
    "walker = pyg.walk(df, spec=\"./page_timelines.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
