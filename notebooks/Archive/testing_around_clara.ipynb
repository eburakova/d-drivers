{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries & Setup dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/nlp_features_abstract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['h1', 'abstract', 'meta_title', 'meta_description', 'merged_url',\n",
       "       'h1_000', 'h1_001', 'h1_02', 'h1_03', 'h1_04',\n",
       "       ...\n",
       "       'abstract_übt', 'abstract_übungen', 'abstract_üppig', 'abstract_üppige',\n",
       "       'abstract_üppigen', 'abstract_üppiger', 'abstract_üppiges',\n",
       "       'abstract_ür', 'abstract_ško', 'abstract_škoda'],\n",
       "      dtype='object', length=35911)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/discover_2024-03-26.xlsx'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'discover_2024-03-26.xlsx'\n",
    "file_path = '../data/' + data_file\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_path, sheet_name='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep raw data for later\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_efahrer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>published_at</th>\n",
       "      <th>publish_date_equal_to_date</th>\n",
       "      <th>page_canonical_url</th>\n",
       "      <th>page_name</th>\n",
       "      <th>classification_product</th>\n",
       "      <th>classification_type</th>\n",
       "      <th>title</th>\n",
       "      <th>page_author</th>\n",
       "      <th>daily_likes</th>\n",
       "      <th>daily_dislikes</th>\n",
       "      <th>word_count</th>\n",
       "      <th>video_play</th>\n",
       "      <th>impressions</th>\n",
       "      <th>discover_clicks</th>\n",
       "      <th>discover_impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010803</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>NaT</td>\n",
       "      <td>N</td>\n",
       "      <td>https://efahrer.chip.de/news/tariferhoehungen-...</td>\n",
       "      <td>efa-1010803 | Tariferhöhungen und THG-Prämie: ...</td>\n",
       "      <td>THG</td>\n",
       "      <td>News</td>\n",
       "      <td>Tariferhöhungen und THG-Prämie: Ladesäulenbet...</td>\n",
       "      <td>Karl Lüdecke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>20323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010592</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>NaT</td>\n",
       "      <td>N</td>\n",
       "      <td>https://efahrer.chip.de/news/das-logo-von-alfa...</td>\n",
       "      <td>efa-1010592 | Alfa Romeo: Was bedeuten Schlang...</td>\n",
       "      <td>Auto</td>\n",
       "      <td>News</td>\n",
       "      <td>Alfa Romeo: Was bedeuten Schlange und Kreuz?</td>\n",
       "      <td>Karl Müller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>286.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010719</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>NaT</td>\n",
       "      <td>N</td>\n",
       "      <td>https://efahrer.chip.de/news/titel-ist-zurueck...</td>\n",
       "      <td>efa-1010719 | Rennen um die effizienteste Sola...</td>\n",
       "      <td>Solaranlagen</td>\n",
       "      <td>News</td>\n",
       "      <td>Rennen um die effizienteste Solarzelle: Deuts...</td>\n",
       "      <td>Aslan Berse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>4912.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010727</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>NaT</td>\n",
       "      <td>N</td>\n",
       "      <td>https://efahrer.chip.de/news/entlastungen-fuer...</td>\n",
       "      <td>efa-1010727 | Antrag stellen oder leer ausgehe...</td>\n",
       "      <td>Energie</td>\n",
       "      <td>Ratgeber</td>\n",
       "      <td>Antrag stellen oder leer ausgehen: Diese Entl...</td>\n",
       "      <td>CHIP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>14009.0</td>\n",
       "      <td>92422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010557</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>Y</td>\n",
       "      <td>https://efahrer.chip.de/news/solaranlage-auch-...</td>\n",
       "      <td>efa-1010557 | Balkonkraftwerk kaufen: Das sind...</td>\n",
       "      <td>Balkonkraftwerk</td>\n",
       "      <td>Kaufberatung</td>\n",
       "      <td>Balkonkraftwerk kaufen: Das sind die besten M...</td>\n",
       "      <td>Eva Goldschald</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>6494.0</td>\n",
       "      <td>114984.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_efahrer_id       date published_at publish_date_equal_to_date   \n",
       "0          1010803 2023-01-02          NaT                          N  \\\n",
       "1          1010592 2023-01-02          NaT                          N   \n",
       "2          1010719 2023-01-05          NaT                          N   \n",
       "3          1010727 2023-01-05          NaT                          N   \n",
       "4          1010557 2023-01-02   2023-01-02                          Y   \n",
       "\n",
       "                                  page_canonical_url   \n",
       "0  https://efahrer.chip.de/news/tariferhoehungen-...  \\\n",
       "1  https://efahrer.chip.de/news/das-logo-von-alfa...   \n",
       "2  https://efahrer.chip.de/news/titel-ist-zurueck...   \n",
       "3  https://efahrer.chip.de/news/entlastungen-fuer...   \n",
       "4  https://efahrer.chip.de/news/solaranlage-auch-...   \n",
       "\n",
       "                                           page_name classification_product   \n",
       "0  efa-1010803 | Tariferhöhungen und THG-Prämie: ...                    THG  \\\n",
       "1  efa-1010592 | Alfa Romeo: Was bedeuten Schlang...                   Auto   \n",
       "2  efa-1010719 | Rennen um die effizienteste Sola...           Solaranlagen   \n",
       "3  efa-1010727 | Antrag stellen oder leer ausgehe...                Energie   \n",
       "4  efa-1010557 | Balkonkraftwerk kaufen: Das sind...        Balkonkraftwerk   \n",
       "\n",
       "  classification_type                                              title   \n",
       "0                News   Tariferhöhungen und THG-Prämie: Ladesäulenbet...  \\\n",
       "1                News       Alfa Romeo: Was bedeuten Schlange und Kreuz?   \n",
       "2                News   Rennen um die effizienteste Solarzelle: Deuts...   \n",
       "3            Ratgeber   Antrag stellen oder leer ausgehen: Diese Entl...   \n",
       "4        Kaufberatung   Balkonkraftwerk kaufen: Das sind die besten M...   \n",
       "\n",
       "      page_author  daily_likes  daily_dislikes  word_count  video_play   \n",
       "0    Karl Lüdecke          NaN             NaN         NaN      1261.0  \\\n",
       "1     Karl Müller          NaN             NaN         NaN       286.0   \n",
       "2     Aslan Berse          NaN             NaN         NaN       156.0   \n",
       "3            CHIP          NaN             NaN         NaN        16.0   \n",
       "4  Eva Goldschald         17.0             1.0      1513.0       174.0   \n",
       "\n",
       "   impressions  discover_clicks  discover_impressions  \n",
       "0       1375.0           1301.0               20323.0  \n",
       "1        298.0            164.0                1493.0  \n",
       "2        300.0            303.0                4912.0  \n",
       "3         55.0          14009.0               92422.0  \n",
       "4        128.0           6494.0              114984.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform columns to lower columns\n",
    "columns = [col.lower() for col in df.columns]\n",
    "df.columns = columns\n",
    "\n",
    "# show results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_efahrer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>published_at</th>\n",
       "      <th>daily_likes</th>\n",
       "      <th>daily_dislikes</th>\n",
       "      <th>word_count</th>\n",
       "      <th>video_play</th>\n",
       "      <th>impressions</th>\n",
       "      <th>discover_clicks</th>\n",
       "      <th>discover_impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.328460e+05</td>\n",
       "      <td>132846</td>\n",
       "      <td>42111</td>\n",
       "      <td>33623.000000</td>\n",
       "      <td>27291.000000</td>\n",
       "      <td>41639.000000</td>\n",
       "      <td>132070.000000</td>\n",
       "      <td>132070.000000</td>\n",
       "      <td>1.320700e+05</td>\n",
       "      <td>1.320700e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.033891e+05</td>\n",
       "      <td>2023-08-11 17:57:35.291089408</td>\n",
       "      <td>2023-11-03 22:30:28.595853568</td>\n",
       "      <td>3.590548</td>\n",
       "      <td>2.693525</td>\n",
       "      <td>665.424986</td>\n",
       "      <td>1441.797108</td>\n",
       "      <td>1836.826244</td>\n",
       "      <td>2.735705e+03</td>\n",
       "      <td>3.147464e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.037000e+03</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2019-02-18 00:00:00</td>\n",
       "      <td>-84.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.010317e+06</td>\n",
       "      <td>2023-05-05 00:00:00</td>\n",
       "      <td>2023-09-06 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>2.810000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.012469e+06</td>\n",
       "      <td>2023-08-03 00:00:00</td>\n",
       "      <td>2023-12-13 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>1.490000e+02</td>\n",
       "      <td>2.148000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.014952e+06</td>\n",
       "      <td>2023-11-23 00:00:00</td>\n",
       "      <td>2024-01-29 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>689.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>628.000000</td>\n",
       "      <td>9.487500e+02</td>\n",
       "      <td>1.230675e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.018782e+06</td>\n",
       "      <td>2024-03-23 00:00:00</td>\n",
       "      <td>2024-03-21 00:00:00</td>\n",
       "      <td>2568.000000</td>\n",
       "      <td>2629.000000</td>\n",
       "      <td>5306.000000</td>\n",
       "      <td>703622.000000</td>\n",
       "      <td>708360.000000</td>\n",
       "      <td>1.053606e+06</td>\n",
       "      <td>1.088435e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.836759e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.784864</td>\n",
       "      <td>42.709102</td>\n",
       "      <td>495.811902</td>\n",
       "      <td>8957.003037</td>\n",
       "      <td>10127.957168</td>\n",
       "      <td>1.542345e+04</td>\n",
       "      <td>1.588974e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_efahrer_id                           date   \n",
       "count     1.328460e+05                         132846  \\\n",
       "mean      8.033891e+05  2023-08-11 17:57:35.291089408   \n",
       "min       1.037000e+03            2023-01-01 00:00:00   \n",
       "25%       1.010317e+06            2023-05-05 00:00:00   \n",
       "50%       1.012469e+06            2023-08-03 00:00:00   \n",
       "75%       1.014952e+06            2023-11-23 00:00:00   \n",
       "max       1.018782e+06            2024-03-23 00:00:00   \n",
       "std       3.836759e+05                            NaN   \n",
       "\n",
       "                        published_at   daily_likes  daily_dislikes   \n",
       "count                          42111  33623.000000    27291.000000  \\\n",
       "mean   2023-11-03 22:30:28.595853568      3.590548        2.693525   \n",
       "min              2019-02-18 00:00:00    -84.000000      -59.000000   \n",
       "25%              2023-09-06 00:00:00      0.000000        0.000000   \n",
       "50%              2023-12-13 00:00:00      0.000000        0.000000   \n",
       "75%              2024-01-29 00:00:00      1.000000        1.000000   \n",
       "max              2024-03-21 00:00:00   2568.000000     2629.000000   \n",
       "std                              NaN     38.784864       42.709102   \n",
       "\n",
       "         word_count     video_play    impressions  discover_clicks   \n",
       "count  41639.000000  132070.000000  132070.000000     1.320700e+05  \\\n",
       "mean     665.424986    1441.797108    1836.826244     2.735705e+03   \n",
       "min      100.000000       0.000000       0.000000     0.000000e+00   \n",
       "25%      415.000000       7.000000      20.000000     2.500000e+01   \n",
       "50%      528.000000      54.000000     107.000000     1.490000e+02   \n",
       "75%      689.000000     379.000000     628.000000     9.487500e+02   \n",
       "max     5306.000000  703622.000000  708360.000000     1.053606e+06   \n",
       "std      495.811902    8957.003037   10127.957168     1.542345e+04   \n",
       "\n",
       "       discover_impressions  \n",
       "count          1.320700e+05  \n",
       "mean           3.147464e+04  \n",
       "min            5.000000e+01  \n",
       "25%            2.810000e+02  \n",
       "50%            2.148000e+03  \n",
       "75%            1.230675e+04  \n",
       "max            1.088435e+07  \n",
       "std            1.588974e+05  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6899,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.page_efahrer_id.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a better understanding of the features\n",
    "\n",
    "Is the ID in PAGE_EFAHRER_ID the same as the ID in PAGE_NAME?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data type for the containment check\n",
    "df['page_efahrer_id'] = df['page_efahrer_id'].astype('string')\n",
    "df['page_name'] = df['page_name'].astype('string')\n",
    "\n",
    "# Function to check whether Page ID is part of page name\n",
    "def check_containment(row):\n",
    "    return row['page_efahrer_id'] in row['page_name']\n",
    "\n",
    "df['containment_check'] = df.apply(check_containment, axis=1)\n",
    "\n",
    "# Check whether results consist only True values\n",
    "df['containment_check'].unique()\n",
    "\n",
    "# Yes, the IDs are always the same\n",
    "\n",
    "# Drop row and transform data type back\n",
    "df.drop('containment_check', axis=1, inplace=True)\n",
    "df['page_efahrer_id'] = df['page_efahrer_id'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page_efahrer_id                    0\n",
       "date                               0\n",
       "published_at                   90735\n",
       "publish_date_equal_to_date         0\n",
       "page_canonical_url                 0\n",
       "page_name                          0\n",
       "classification_product           655\n",
       "classification_type              655\n",
       "title                              0\n",
       "page_author                        0\n",
       "daily_likes                    99223\n",
       "daily_dislikes                105555\n",
       "word_count                     91207\n",
       "video_play                       776\n",
       "impressions                      776\n",
       "discover_clicks                  776\n",
       "discover_impressions             776\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish date \"published_at\" can be imputed by setting publish date to first date of occurrence in the data set.\n",
    "Classification Type \"classification_type\" can be imputed by extracting it from URL.\n",
    "\n",
    "\n",
    "# TBD IMPUTING OF MISSING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape data (tittles, actual date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://efahrer.chip.de/news/engea-im-schnellcheck-wirklich-deutschlands-komfortabelste-wallbox-loesung_1011849'\n",
    "\n",
    "# html = requests.get(url)\n",
    "\n",
    "# soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for row_idx in scraping.index:\n",
    "    url = scraping.loc[row_idx, 'page_canonical_url']\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    ### meta data ###\n",
    "    # meta title, displayed in search results\n",
    "    scraping.loc[row_idx, 'title'] = soup.find('title').text\n",
    "    # meta description, displayed in search results\n",
    "    scraping.loc[row_idx, 'meta_description'] = soup.find('meta', attrs={'name': 'description'}).get('content')\n",
    "    # meta image, primarily for social media, but might be displayed in search results\n",
    "    scraping.loc[row_idx, 'meta_image_url'] = soup.find('meta', attrs={'property': 'og:image'}).get('content')\n",
    "\n",
    "    # media type for first big image/video on page\n",
    "    h4_element = soup.find('h4', class_='mt-0 credentials open-sans-regular')  # Find the h4 element with specific class\n",
    "    if h4_element:\n",
    "        next_div = h4_element.find_next('div')  # Find the next div element after the h4\n",
    "        scraping.loc[row_idx, 'media_type'] = next_div.get('class') if next_div else None  # Get the class attribute of the next div\n",
    "        if 'img-wrapper' in media_type:\n",
    "            scraping.loc[row_idx, 'media_type'] = 'img'\n",
    "        elif 'mb-3 video-player recobar' in media_type:\n",
    "            scraping.loc[row_idx, 'media_type'] = 'video'\n",
    "        else:\n",
    "            scraping.loc[row_idx, 'media_type'] = media_type\n",
    "            # VIDEO = {'class'='mb-3 video-player recobar'}\n",
    "            # IMAGE = {'class'='img-wrapper'}\n",
    "\n",
    "    # image size\n",
    "    if scraping.loc[row_idx, 'media_type'] == 'img':\n",
    "    #    page_img_url = soup.find(id='content').find('article').find('div', {'class':'img-wrapper'}).find('img').get('src').text\n",
    "        scraping.loc[row_idx, 'page_img_size'] = soup.find(id='content').find('article').find('div', {'class':'img-wrapper'}).find('img').get('sizes')\n",
    "\n",
    "\n",
    "    ### user-visible data ###\n",
    "    # first headline on the article page\n",
    "    scraping.loc[row_idx, 'h1'] = soup.find('h1').text\n",
    "    # author displayed on the article page\n",
    "    scraping.loc[row_idx, 'author'] = soup.find(id='content').find('article').find('h4').find('a').text\n",
    "    # date displayed on the article page\n",
    "    scraping.loc[row_idx, 'date'] = soup.find(id='content').find('article').find('h4').find('span').text\n",
    "    # abstract, first text paragraph of the article\n",
    "    scraping.loc[row_idx, 'abstract'] = soup.find(id='content').find('article').find('p').text\n",
    "\n",
    "    #h4 = soup.find_all('h4', {'class': 'mt-0 credentials open-sans-regular'})\n",
    "    #img = soup.find(\"article\", {\"class\": \"single-article\"}).find('div', {'class': 'img-wrapper'}).find(\"div\", {'class': 'caption'}).find(\"span\", {\"class\": \"p img-title\"}).text\n",
    "\n",
    "    i+=1\n",
    "    if i==10: \n",
    "        scraping.to_csv('../data/temp_scraped.csv')\n",
    "        i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file: ../data/pages/index.html, 'NoneType' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing HTML files\n",
    "folder_path = '../data/pages'\n",
    "\n",
    "# Initialize a list to hold the scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Iterate over HTML files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.html'):  # Check if the file is an HTML file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                # Extract meta data\n",
    "                url = soup.find('link', {'rel':'canonical'}).get('href')\n",
    "                meta_title = soup.find('title').text\n",
    "                meta_description = soup.find('meta', attrs={'name': 'description'}).get('content')\n",
    "                meta_image_url = soup.find('meta', attrs={'property': 'og:image'}).get('content')\n",
    "\n",
    "                # Extract media type\n",
    "                media_type_element = soup.find('h4', class_='mt-0 credentials open-sans-regular').find_next('div')\n",
    "                media_type_class = media_type_element.get('class') if media_type_element else None\n",
    "                media_type = 'img' if 'img-wrapper' in media_type_class else 'video' if 'mb-3 video-player recobar' in media_type_class else None\n",
    "\n",
    "                # Extract image size\n",
    "                page_img_size = None\n",
    "                if media_type == 'img':\n",
    "                    page_img_size = soup.find(id='content').find('article').find('div', {'class':'img-wrapper'}).find('img').get('sizes')\n",
    "\n",
    "                # Extract user-visible data\n",
    "                h1 = soup.find('h1').text\n",
    "                author = soup.find(id='content').find('article').find('h4').find('a').text\n",
    "                date = soup.find(id='content').find('article').find('h4').find('span').text\n",
    "                abstract = soup.find(id='content').find('article').find('p').text\n",
    "\n",
    "                # Append scraped data to the list\n",
    "                scraped_data.append({\n",
    "                    'filename': filename,                     \n",
    "                    'url': url,\n",
    "                    'meta_title': meta_title,\n",
    "                    'meta_description': meta_description,\n",
    "                    'meta_image_url': meta_image_url,\n",
    "                    'media_type': media_type,\n",
    "                    'page_img_size': page_img_size,\n",
    "                    'h1': h1,\n",
    "                    'author': author,\n",
    "                    'date': date,\n",
    "                    'abstract': abstract\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {file_path}, {e}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "scraped_df.to_csv('../data/temp_scraped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filename'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# processing of the scraped data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m scraped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mscraped_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m scraped_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m scraped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_title\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scraped_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'filename'"
     ]
    }
   ],
   "source": [
    "# processing of the scraped data\n",
    "scraped_df['page_ID'] = scraped_df['filename'].apply(lambda x: x.split('.')[0])\n",
    "scraped_df.drop('filename', axis=1, inplace=True)\n",
    "\n",
    "scraped_df['meta_title'] = scraped_df['meta_title'].apply(lambda x: x.rsplit('-', 1)[0])\n",
    "\n",
    "# Apply strip() method to remove leading and trailing whitespaces from all string columns\n",
    "scraped_df = scraped_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "#scraped_df['date'] = pd.to_datetime(scraped_df['date'], format='%d. %B %Y')\n",
    "scraped_df['date'] = pd.to_datetime(scraped_df['date'], errors='coerce')\n",
    "\n",
    "scraped_df['page_img_size'] = scraped_df['page_img_size'].apply(lambda x: x.split(',')[0] if x else None)\n",
    "scraped_df['page_img_size'] = scraped_df['page_img_size'].apply(lambda x: x.split(')')[-1] if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_efahrer_id</th>\n",
       "      <th>page_canonical_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010803</td>\n",
       "      <td>https://efahrer.chip.de/news/tariferhoehungen-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010592</td>\n",
       "      <td>https://efahrer.chip.de/news/das-logo-von-alfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010719</td>\n",
       "      <td>https://efahrer.chip.de/news/titel-ist-zurueck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010727</td>\n",
       "      <td>https://efahrer.chip.de/news/entlastungen-fuer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010557</td>\n",
       "      <td>https://efahrer.chip.de/news/solaranlage-auch-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130737</th>\n",
       "      <td>1016319</td>\n",
       "      <td>https://efahrer.chip.de/news/elektro-beliebter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131077</th>\n",
       "      <td>1010895</td>\n",
       "      <td>https://efahrer.chip.de/news/nie-wieder-kabels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131188</th>\n",
       "      <td>1018743</td>\n",
       "      <td>https://efahrer.chip.de/news/irren-pfusch-am-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131479</th>\n",
       "      <td>1017718</td>\n",
       "      <td>https://efahrer.chip.de/news/alle-59-meter-ein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131864</th>\n",
       "      <td>109080</td>\n",
       "      <td>https://efahrer.chip.de/news/tesla-als-kuehlsc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6899 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        page_efahrer_id                                 page_canonical_url\n",
       "0               1010803  https://efahrer.chip.de/news/tariferhoehungen-...\n",
       "1               1010592  https://efahrer.chip.de/news/das-logo-von-alfa...\n",
       "2               1010719  https://efahrer.chip.de/news/titel-ist-zurueck...\n",
       "3               1010727  https://efahrer.chip.de/news/entlastungen-fuer...\n",
       "4               1010557  https://efahrer.chip.de/news/solaranlage-auch-...\n",
       "...                 ...                                                ...\n",
       "130737          1016319  https://efahrer.chip.de/news/elektro-beliebter...\n",
       "131077          1010895  https://efahrer.chip.de/news/nie-wieder-kabels...\n",
       "131188          1018743  https://efahrer.chip.de/news/irren-pfusch-am-e...\n",
       "131479          1017718  https://efahrer.chip.de/news/alle-59-meter-ein...\n",
       "131864           109080  https://efahrer.chip.de/news/tesla-als-kuehlsc...\n",
       "\n",
       "[6899 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique ids\n",
    "scraping = df[[\"page_efahrer_id\", \"page_canonical_url\"]].drop_duplicates(subset = [\"page_efahrer_id\"])\n",
    "scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_canonical_url</th>\n",
       "      <th>H1</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page_efahrer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010803</th>\n",
       "      <td>https://efahrer.chip.de/news/tariferhoehungen-...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010592</th>\n",
       "      <td>https://efahrer.chip.de/news/das-logo-von-alfa...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010719</th>\n",
       "      <td>https://efahrer.chip.de/news/titel-ist-zurueck...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010727</th>\n",
       "      <td>https://efahrer.chip.de/news/entlastungen-fuer...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010557</th>\n",
       "      <td>https://efahrer.chip.de/news/solaranlage-auch-...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016319</th>\n",
       "      <td>https://efahrer.chip.de/news/elektro-beliebter...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010895</th>\n",
       "      <td>https://efahrer.chip.de/news/nie-wieder-kabels...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018743</th>\n",
       "      <td>https://efahrer.chip.de/news/irren-pfusch-am-e...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017718</th>\n",
       "      <td>https://efahrer.chip.de/news/alle-59-meter-ein...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109080</th>\n",
       "      <td>https://efahrer.chip.de/news/tesla-als-kuehlsc...</td>\n",
       "      <td>dummy</td>\n",
       "      <td>today</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6899 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                page_canonical_url     H1   \n",
       "page_efahrer_id                                                             \n",
       "1010803          https://efahrer.chip.de/news/tariferhoehungen-...  dummy  \\\n",
       "1010592          https://efahrer.chip.de/news/das-logo-von-alfa...  dummy   \n",
       "1010719          https://efahrer.chip.de/news/titel-ist-zurueck...  dummy   \n",
       "1010727          https://efahrer.chip.de/news/entlastungen-fuer...  dummy   \n",
       "1010557          https://efahrer.chip.de/news/solaranlage-auch-...  dummy   \n",
       "...                                                            ...    ...   \n",
       "1016319          https://efahrer.chip.de/news/elektro-beliebter...  dummy   \n",
       "1010895          https://efahrer.chip.de/news/nie-wieder-kabels...  dummy   \n",
       "1018743          https://efahrer.chip.de/news/irren-pfusch-am-e...  dummy   \n",
       "1017718          https://efahrer.chip.de/news/alle-59-meter-ein...  dummy   \n",
       "109080           https://efahrer.chip.de/news/tesla-als-kuehlsc...  dummy   \n",
       "\n",
       "                last_update_date abstract  \n",
       "page_efahrer_id                            \n",
       "1010803                    today     null  \n",
       "1010592                    today     null  \n",
       "1010719                    today     null  \n",
       "1010727                    today     null  \n",
       "1010557                    today     null  \n",
       "...                          ...      ...  \n",
       "1016319                    today     null  \n",
       "1010895                    today     null  \n",
       "1018743                    today     null  \n",
       "1017718                    today     null  \n",
       "109080                     today     null  \n",
       "\n",
       "[6899 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the dummy columns that will be populated with the scraped data\n",
    "scraping['H1'] = 'dummy'\n",
    "scraping['last_update_date'] = 'today'\n",
    "scraping['abstract'] = 'null'\n",
    "\n",
    "scraping = scraping.set_index('page_efahrer_id')\n",
    "\n",
    "scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m scraping\u001b[38;5;241m.\u001b[39mloc[row_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m----> 9\u001b[0m scraping\u001b[38;5;241m.\u001b[39mloc[row_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_update_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m     10\u001b[0m scraping\u001b[38;5;241m.\u001b[39mloc[row_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle-article\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     11\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "## Do actual scraping\n",
    "i=0\n",
    "for row_idx in scraping.index:\n",
    "    url = scraping.loc[row_idx, 'page_canonical_url']\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    scraping.loc[row_idx, 'H1'] = soup.find('h1').text\n",
    "    scraping.loc[row_idx, 'last_update_date'] = soup.find('h4').find('span').text\n",
    "    scraping.loc[row_idx, 'abstract'] = soup.find(\"article\", {\"class\": \"single-article\"}).find('p').text\n",
    "\n",
    "    scraping.loc[row_idx, 'abstract'] = soup.find(\"article\", {\"class\": \"single-article\"}).find('p').text\n",
    "\n",
    "\n",
    "    i+=1\n",
    "    if i==10: \n",
    "        scraping.to_csv('../data/temp_scraped.csv')\n",
    "        i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the date\n",
    "# Find the span element within the h4 tag\n",
    "date_span = soup.find('h4').find('span')\n",
    "\n",
    "# Extract the text containing the date\n",
    "date_text = date_span.text\n",
    "\n",
    "abstract_p = soup.find('p')\n",
    "\n",
    "# Extract the text containing the date\n",
    "abstract_text = abstract_p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning to be done\n",
    "\n",
    "1. getting only the last part of the URL\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/discover_2024-03-26.xlsx'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_file = 'discover_2024-03-26.xlsx'\n",
    "file_path = '../data/' + data_file\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_path, sheet_name='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be sorted/ organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub = df.query(\"publish_date_equal_to_date == 'Y'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.page_efahrer_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub.page_efahrer_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.daily_likes.isna().sum())\n",
    "print(df.daily_likes.notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"page_efahrer_id\").daily_likes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_likes = df.daily_likes.\n",
    "daily_likes\n",
    "#sns.histplot(df.daily_likes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_likes = df['daily_likes'].dropna()\n",
    "non_null_likes.unique().min()\n",
    "#sns.histplot(non_null_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.page_efahrer_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"page_efahrer_id\", \"date\", \"DISCOVER_IMPRESSIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"page_efahrer_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"PAGE_EFAHRER_ID\", \"PAGE_NAME\"]].drop_duplicates().groupby(\"PAGE_NAME\").count().max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
