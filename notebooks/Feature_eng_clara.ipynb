{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/aggr_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_perf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/aggr_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_scrape \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/full_scraped.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m df_perf\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mold_index\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublish_date_equal_to_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion_id\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/neuefische/d-drivers/.venv/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/aggr_data.csv'"
     ]
    }
   ],
   "source": [
    "df_perf = pd.read_csv('data/aggr_data.csv')\n",
    "df_scrape = pd.read_csv('data/full_scraped.csv')\n",
    "\n",
    "df_perf.drop(['old_index','publish_date_equal_to_date','version_id'], axis=1, inplace=True)\n",
    "df_perf = df_perf[['page_version_id','page_id','version_id_new','date', 'publish_date', 'word_count', 'url', 'page_name','title',\n",
    "       'classification_product', 'classification_type', 'authors','daily_likes',\n",
    "       'daily_dislikes', 'video_play', 'page_impressions', 'clickouts',\n",
    "       'external_clicks', 'external_impressions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variables:\n",
    "Impressions, Clicks, CTR (click-through-rate). The latter we create in the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Click through rate based on external clicks and impressions\n",
    "df_perf['ctr'] = df_perf['external_clicks'] / df_perf['external_impressions'] *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and feature engineering of scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the last part of the URL to analyze it and inhibit duplicate data with classification_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract last part of URL and clean it\n",
    "def extract_last_part(url):\n",
    "    url_text = url.rsplit('/', 1)[-1]\n",
    "    cleaned_url = url_text.split('_')[0]\n",
    "    cleaned_url_list = cleaned_url.split('-')\n",
    "    return cleaned_url_list\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df_scrape['url_text'] = df_scrape['url'].apply(extract_last_part)\n",
    "\n",
    "# Sum up all list items per ongoing Version ID and merge with original df\n",
    "df_feat = pd.merge(df_scrape, df_scrape.groupby('page_id')['url_text'].apply(lambda x: list(set(sum(x, [])))).reset_index(name='merged_url'), on='page_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform media column\n",
    "def media_type(df, media_type):\n",
    "    if 'img-wrapper' in media_type or any(item in media_type for item in ['image-gallery', 'mb-lg-7', 'mb-8']):\n",
    "        return 'img'\n",
    "    elif any(item in media_type for item in ['mb-3', 'video-player', 'recobar']):\n",
    "        return 'video'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df_feat['media_type'] = df_scrape['media_type'].apply(lambda x: media_type(df_feat, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title length\n",
    "df_feat['meta_title_len'] = df_feat['meta_title'].str.len()\n",
    "\n",
    "# Meta description length\n",
    "df_feat['meta_desc_len'] = df_feat['meta_description'].str.len()\n",
    "\n",
    "# H1 length\n",
    "df_feat['h1_len'] = df_feat['h1'].str.len()\n",
    "\n",
    "# Abstract length\n",
    "df_feat['abstract_len'] = df_feat['abstract'].str.len()\n",
    "\n",
    "# URL length\n",
    "df_feat['merged_url_len'] = df_feat['merged_url'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['page_id', 'url', 'h1', 'author', 'date_scraped', 'abstract',\n",
      "       'main_text_length', 'meta_title', 'meta_description', 'meta_image_url',\n",
      "       'media_type', 'page_img_size', 'url_text', 'merged_url',\n",
      "       'meta_title_len', 'meta_desc_len', 'h1_len', 'abstract_len',\n",
      "       'merged_url_len'],\n",
      "      dtype='object')\n",
      "Index(['page_version_id', 'page_id', 'version_id_new', 'date', 'publish_date',\n",
      "       'word_count', 'url', 'page_name', 'title', 'classification_product',\n",
      "       'classification_type', 'authors', 'daily_likes', 'daily_dislikes',\n",
      "       'video_play', 'page_impressions', 'clickouts', 'external_clicks',\n",
      "       'external_impressions', 'ctr'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_feat.columns)\n",
    "print(df_perf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging scraped and provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_keys = ['page_id', 'url']\n",
    "df_full = pd.merge(left=df_perf,right=df_feat,how='left',on=merge_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['page_version_id', 'page_id', 'version_id_new', 'date', 'publish_date',\n",
       "       'word_count', 'url', 'page_name', 'title', 'classification_product',\n",
       "       'classification_type', 'authors', 'daily_likes', 'daily_dislikes',\n",
       "       'video_play', 'page_impressions', 'clickouts', 'external_clicks',\n",
       "       'external_impressions', 'ctr', 'h1', 'author', 'date_scraped',\n",
       "       'abstract', 'main_text_length', 'meta_title', 'meta_description',\n",
       "       'meta_image_url', 'media_type', 'page_img_size', 'url_text',\n",
       "       'merged_url', 'meta_title_len', 'meta_desc_len', 'h1_len',\n",
       "       'abstract_len', 'merged_url_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>to be done\n",
    "### Finetune versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_perf = ['page_id', 'page_version_id','date','external_clicks', 'external_impressions','video_play', 'page_impressions', 'clickouts','daily_likes', 'daily_dislikes','ctr']\n",
    "\n",
    "col_gen = ['page_version_id', 'page_id', 'version_id_new', 'publish_date', 'word_count', 'url', 'page_name', 'title', 'classification_product', 'classification_type', 'authors']\n",
    "\n",
    "# Define aggregation rules for aggregation by date\n",
    "agg_to_date = {\n",
    "    'page_version_id': 'sum', # Number of versions\n",
    "    'page_id': 'first', # Id for matching\n",
    "    #'date': 'first', # Prevent error\n",
    "    'external_impressions': 'first',  # Metric is duplicated\n",
    "    'external_clicks': 'first',   # Metric is duplicated\n",
    "    'video_play': 'sum',   # Metric sometimes differs highly for different URLs & authors\n",
    "    'page_impressions': 'sum',   # Metric sometimes differs highly for different URLs & authors\n",
    "    'clickouts': 'sum',   # Metric sometimes differs highly for different URLs & authors\n",
    "    'daily_likes': 'first',   # Metric is duplicated\n",
    "    'daily_dislikes': 'first',   # Metric is duplicated\n",
    "    'ctr': 'first' # Metric is duplicated\n",
    "}\n",
    "\n",
    "# Aggregate data on a date level\n",
    "df_perf_date = df_perf[col_perf].groupby(['page_id','date'],as_index=False).agg(agg_to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_date\n",
    "#df_perf_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define aggregation rules for aggregation by page_id\n",
    "agg_to_page = {\n",
    "    'page_version_id': 'first', # Id for matching\n",
    "    'page_id': 'first',\n",
    "    'date': 'first', # Prevent error\n",
    "    'external_impressions': 'sum',  # Sum of all days\n",
    "    'external_clicks': 'sum',   # Sum of all days\n",
    "    'video_play': 'sum',   # Sum of all days\n",
    "    'page_impressions': 'sum',   # Sum of all days\n",
    "    'clickouts': 'sum',   # Sum of all days\n",
    "    'daily_likes': 'sum',   # Sum of all days\n",
    "    'daily_dislikes': 'sum',   # Sum of all days\n",
    "    'ctr': 'median' # Meidan of all days # Could be recalculated alternatively\n",
    "}\n",
    "\n",
    "# Aggregate on a page_id level\n",
    "df_perf_page = df_perf_date[col_perf].groupby(['page_id']).agg(agg_to_page)\n",
    "\n",
    "print(df_perf_date.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen_page = df_perf[col_gen].groupby(['page_id']).first()\n",
    "\n",
    "df_agg = pd.merge(left=df_perf_date, right=df_gen_page, on='page_id',how='left')\n",
    "\n",
    "# df_agg = df_agg[['date','publish_date', 'word_count', 'url', 'page_name', 'title',\n",
    "#        'classification_product', 'classification_type', 'authors', \n",
    "#        'page_id','version_id_new', 'video_play', 'page_impressions', 'clickouts', 'daily_likes',\n",
    "#        'daily_dislikes', 'ctr',  'external_impressions', 'external_clicks']]\n",
    "\n",
    "print(df_gen_page.shape)\n",
    "print(df_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create ongoing version_id that is unique for each version page_id combination\n",
    "# df_perf['version_id_ong'] = df_perf['page_id'].astype(str) + '_' + df_perf['version_id_new'].astype(str)\n",
    "# df_perf[['version_id_ong', 'page_id', 'version_id_new']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thomas:\n",
    "We have three groups which depend on different concatenated unique keys:\n",
    "\n",
    "#### Group 1: external_impressions and external_clicks:\n",
    "\n",
    "page_id\n",
    "date\n",
    "\n",
    "#### Group 2: video_play, page_impressions, clickouts:\n",
    "\n",
    "page_id\n",
    "date\n",
    "URL\n",
    "Author (edge case)\n",
    "\n",
    "#### Group 3: daily_likes, daily_dislikes:\n",
    "\n",
    "page_id\n",
    "date\n",
    "publishe_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_agg_1 = ['external_clicks', 'external_impressions']\n",
    "col_agg_2 = ['video_play', 'page_impressions', 'clickouts']\n",
    "col_agg_3 = ['daily_likes', 'daily_dislikes']\n",
    "\n",
    "# Columns that don't need to be aggregated but are the same for each version\n",
    "all_columns = df_perf.columns.tolist()\n",
    "col = [c for c in all_columns if c not in col_agg_1 and c not in col_agg_2 and c not in col_agg_3]\n",
    "# this includes: ['old_index', 'page_id', 'date', 'url', 'version_id_new', 'publish_date', 'word_count', 'words_scraped', 'classification_product', 'classification_type', 'page_name', 'authors', 'author_scraped', 'title', 'h1', 'abstract', 'last_update', 'image_url', 'version_id_ong']\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 20)\n",
    "\n",
    "# Columns that differ on a daily basis and need to be aggregated with a certain rule\n",
    "col_special = ['page_version_id','date','external_clicks', 'external_impressions','video_play', 'page_impressions', 'clickouts','daily_likes', 'daily_dislikes']\n",
    "\n",
    "# Columns that don't need to be aggregated but are the same for each version\n",
    "all_columns = df_perf.columns.tolist()\n",
    "col = [c for c in all_columns if c not in col_special] + ['page_version_id'] + ['date']\n",
    "# this includes: ['old_index', 'page_id', 'date', 'url', 'version_id_new', 'publish_date', 'word_count', 'words_scraped', 'classification_product', 'classification_type', 'page_name', 'authors', 'author_scraped', 'title', 'h1', 'abstract', 'last_update', 'image_url', 'version_id_ong']\n",
    "\n",
    "# Aggregate by version for columns with simple duplicates\n",
    "df_agg_col = df_perf[col].groupby('page_version_id').first()\n",
    "\n",
    "# Aggregate by version for columns which need to be aggregated with a certain rule\n",
    "agg_funcs = {\n",
    "    'external_impressions': 'first',  # Metric is duplicated\n",
    "    'external_clicks': 'first',   # Metric is duplicated\n",
    "    'video_play': 'sum',   # Metric sometimes differs highly for different URLs & authors\n",
    "    'page_impressions': 'sum',   # Metric sometimes differs highly for different URLs & authors\n",
    "    'clickouts': 'sum',   # Metric sometimes differs highly for different URLs & authors\n",
    "    'daily_likes': 'first',   # Metric is duplicated\n",
    "    'daily_dislikes': 'first'   # Metric is duplicated\n",
    "}\n",
    "\n",
    "# Group by and apply aggregation functions\n",
    "df_agg_special = df_perf[col_special].groupby(['date']).sum()\n",
    "df_agg_special = df_perf[col_special].groupby(['page_version_id']).agg(agg_funcs)\n",
    "\n",
    "df_agg = pd.merge(left=df_agg_col, right=df_agg_special, on='page_version_id',how='left')\n",
    "\n",
    "df_agg = df_agg[['date','publish_date', 'word_count', 'url', 'page_name', 'title',\n",
    "       'classification_product', 'classification_type', 'authors', \n",
    "       'page_id','version_id_new', 'video_play', 'page_impressions', 'clickouts', 'daily_likes',\n",
    "       'daily_dislikes', 'ctr',  'external_impressions', 'external_clicks']]\n",
    "\n",
    "print(df_agg_special.shape)\n",
    "print(df_agg_col.shape)\n",
    "print(df_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
